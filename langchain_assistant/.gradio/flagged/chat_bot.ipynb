{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b99d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create the LLM -> decide on which device to run model currenlty using cpu hence give message sevice set to run on cpu\n",
    "pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Create the prompt template + pass input variable to it\n",
    "prompt = PromptTemplate.from_template(\"Explain {topic} in simple words.\")\n",
    "\n",
    "# RunnableSequence : It‚Äôs not a normal ‚Äúfunction‚Äù, but a LangChain class that behaves like a function\n",
    "# Combine them into one \"runnable sequence\" is is take prompt op and send it to llm \n",
    "chain = RunnableSequence(prompt | local_llm)\n",
    "\n",
    "# Use it like a function\n",
    "response = chain.invoke({\"topic\": \"Who is Virat Kolhi\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f12eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\OneDrive\\Desktop\\Langchain\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_15912\\3740355864.py:27: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFacePipeline``.\n",
      "  local_llm = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "\n",
      "üß† AI Answer:\n",
      " java bean\n"
     ]
    }
   ],
   "source": [
    "# --- üì¶ Imports ---\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "# --- üß© Optional: Hide unnecessary Hugging Face logs (like \"Device set to use CPU\") ---\n",
    "transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "# --- ‚öôÔ∏è Step 1: Load the Local Model ---\n",
    "# Model size options:\n",
    "# \"google/flan-t5-small\" ‚Üí light & fast\n",
    "# \"google/flan-t5-base\"  ‚Üí better answers, moderate size (~1GB)\n",
    "# \"google/flan-t5-large\" ‚Üí more accurate, heavy (~3‚Äì4GB)\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "# Create text2text pipeline (runs on CPU or GPU automatically)\n",
    "pipe = pipeline(\n",
    "    task=\"text2text-generation\",\n",
    "    model=model_name,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,    # Controls creativity: 0.1 = factual, 1.0 = creative\n",
    ")\n",
    "\n",
    "# Wrap pipeline for LangChain compatibility\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "\n",
    "# --- üí¨ Step 2: Create Prompt Template ---\n",
    "# Defines how user input (variable) will be inserted into the question RunnableSequence : It‚Äôs not a normal ‚Äúfunction‚Äù, but a LangChain class that behaves like a function\n",
    "prompt = PromptTemplate.from_template(\"Explain {topic} in simple and short words.\")\n",
    "\n",
    "# --- üîó Step 3: Build a Runnable Sequence ---\n",
    "# Connects prompt ‚Üí model (output of prompt goes as input to our local LLM variable)\n",
    "chain = RunnableSequence(prompt | local_llm)\n",
    "\n",
    "# --- üß† Step 4: Ask a Question (like a function call) ---\n",
    "query = {\"topic\": \"java?\"}\n",
    "response = chain.invoke(query)\n",
    "\n",
    "# --- üñ®Ô∏è Step 5: Display Result ---\n",
    "print(\"\\nüß† AI Answer:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a43243",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.13.5' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "# --- Hide logs ---\n",
    "transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "# --- Load model ---\n",
    "pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", max_new_tokens=250)\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# --- Define prompts ---\n",
    "explain_prompt = PromptTemplate.from_template(\"Explain {topic} in simple words.\")\n",
    "adv_prompt = PromptTemplate.from_template(\"Based on this explanation: {explanation}, list 3 advantages of {topic}.\")\n",
    "example_prompt = PromptTemplate.from_template(\"From the advantages: {advantages}, give one real-world example of {topic}.\")\n",
    "\n",
    "# --- Build sub-chains ---\n",
    "explain_chain = explain_prompt | local_llm\n",
    "adv_chain = adv_prompt | local_llm\n",
    "example_chain = example_prompt | local_llm\n",
    "\n",
    "# --- Connect sequentially ---\n",
    "# Step 1 ‚Üí Step 2 ‚Üí Step 3\n",
    "chain = (\n",
    "    explain_chain\n",
    "    | (lambda text: {\"topic\": \"Java programming language\", \"explanation\": text})\n",
    "    | adv_chain\n",
    "    | (lambda text: {\"topic\": \"Java programming language\", \"advantages\": text})\n",
    "    | example_chain\n",
    ")\n",
    "\n",
    "# --- Run the chain ---\n",
    "response = chain.invoke({\"topic\": \"Java programming language\"})\n",
    "\n",
    "print(\"\\nüß† Final AI Output:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e56d81",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.13.5' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# --- üì¶ Imports ---\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "import gradio as gr\n",
    "\n",
    "# --- Hide unnecessary logs ---\n",
    "transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "# --- Load Local Model ---\n",
    "pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", max_new_tokens=300)\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# --- Define Prompts ---\n",
    "explain_prompt = PromptTemplate.from_template(\"Explain {topic} in simple words.\")\n",
    "adv_prompt = PromptTemplate.from_template(\"Based on this explanation: {explanation}, list 3 advantages of {topic}.\")\n",
    "example_prompt = PromptTemplate.from_template(\"From the advantages: {advantages}, give one real-world example of {topic}.\")\n",
    "\n",
    "# --- Create Subchains ---\n",
    "explain_chain = explain_prompt | local_llm\n",
    "adv_chain = adv_prompt | local_llm\n",
    "example_chain = example_prompt | local_llm\n",
    "\n",
    "# --- Function for Gradio ---\n",
    "def explain_topic(topic):\n",
    "    explanation = explain_chain.invoke({\"topic\": topic})\n",
    "    advantages = adv_chain.invoke({\"topic\": topic, \"explanation\": explanation})\n",
    "    example = example_chain.invoke({\"topic\": topic, \"advantages\": advantages})\n",
    "    return explanation, advantages, example\n",
    "\n",
    "# --- Build Gradio Interface ---\n",
    "demo = gr.Interface(\n",
    "    fn=explain_topic,\n",
    "    inputs=gr.Textbox(label=\"Enter a Topic (e.g., Java, Blockchain, Machine Learning)\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"üí¨ Explanation\"),\n",
    "        gr.Textbox(label=\"üí° Advantages\"),\n",
    "        gr.Textbox(label=\"üß∞ Real-world Example\"),\n",
    "    ],\n",
    "    title=\"LangChain Multi-Step AI Assistant\",\n",
    "    description=\"Type any topic, and the AI will explain it, list advantages, and give one real-world example.\",\n",
    ")\n",
    "\n",
    "# --- Launch the app ---\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LangChain venv)",
   "language": "python",
   "name": "langchain_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
